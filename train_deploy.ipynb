{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Deploys a fastai model to a sagemaker endpoint using torchserve.  This notebook can be run on a CPU based Sagemaker notebook instance.\n",
    "\n",
    "This is based on other guides on the internet that use [pytorch 1.0](https://course19.fast.ai/deployment_amzn_sagemaker.html) and [pytorch 1.4](https://github.com/mattmcclean/fastai2-sagemaker-deployment-demo/blob/master/fastai2_deploy_sagemaker_demo.ipynb).  This guide uses the newer deployment mechanism of torchserve which is only available in pytorch >= 1.6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install fastai deps\n",
    "! pip install -Uqq fastbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check versions\n",
    "! pip list | egrep 'fast|torch|sagemaker'\n",
    "\n",
    "# fastai >= 2.1.9      (we want a modern fastai v2)\n",
    "# sagemaker >= 2.19.0  (should be sagemaker v2, might need to upgrade via pip)\n",
    "# torch >= 1.7.1       (make sure torch meets minimum requirements of fastai)\n",
    "# torchvision >= 0.8.2 (make sure torchvision meets minimum requirements of fastai)\n",
    "\n",
    "# see environment.yml of the fastai version you are using for required dependencies: https://github.com/fastai/fastai/blob/master/environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import tarfile\n",
    "from fastai.vision.all import *\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "# create the image data loader\n",
    "dls = ImageDataLoaders.from_path_re(path, get_image_files(path), pat=r'(.+)_\\d+.jpg$', \n",
    "                                    item_tfms=RandomResizedCrop(460, min_scale=0.75), bs=64,\n",
    "                                    batch_tfms=[*aug_transforms(size=299, max_warp=0),\n",
    "                                    Normalize.from_stats(*imagenet_stats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, keeping it simple in case you're on a CPU instance\n",
    "learn = cnn_learner(dls, resnet18, metrics=error_rate)\n",
    "learn.fine_tune(0, freeze_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file must be named model.pth to work with torchserve running on framework 1.6.0\n",
    "model_export = 'model.pth'\n",
    "\n",
    "# export learner to path\n",
    "# must do it this way so it includes the model weights and architecture\n",
    "# save_model and torch.save will not work\n",
    "learn.export('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where the model.pth file gets saved\n",
    "learn.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate model archive (.tar.gz)\n",
    "# model.pth should be the only file stuffed into model archive\n",
    "path = Path('.')\n",
    "model_archive = 'pets_model.tar.gz'\n",
    "with tarfile.open(path/model_archive, 'w:gz') as f:\n",
    "    f.add(learn.path/model_export, arcname=model_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "prefix = 'DEMO-fastai2-sagemaker-oxford-pets'\n",
    "model_location = sess.upload_data(str(path/model_archive), key_prefix=prefix)\n",
    "model_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up local files\n",
    "Path(path/model_archive).unlink()\n",
    "Path(learn.path/model_export).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy local endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate serializer objects the new sagemaker v2 way\n",
    "json_serializer = JSONSerializer()\n",
    "json_deserializer = JSONDeserializer()\n",
    "\n",
    "# get role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# framework_version=1.6.0 is the latest supported pytorch and the only one compatible with new torchserve deployment model\n",
    "# pytorch 1.7.1 will be installed via requirements.txt\n",
    "# serve.py is the script that will load the model and process predictions\n",
    "\n",
    "model = PyTorchModel(model_data=model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.6.0',\n",
    "                     py_version='py36',\n",
    "                     entry_point='serve.py', \n",
    "                     source_dir='scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test docker container locally\n",
    "# may fail due to lack of space on device\n",
    "# if so, then:\n",
    "#  - clean out all /tmp/tmp* files\n",
    "#  - docker rm all containers (use docker ps -a to list)\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='local',\n",
    "                         serializer=json_serializer, deserializer=json_deserializer, content_type='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference\n",
    "response = predictor.predict( { \"url\": \"https://cdn1-www.cattime.com/assets/uploads/2011/12/file_2744_british-shorthair-460x290-460x290.jpg\" } )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to stop local docker container\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy remote endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out old objects or else model.deploy() will still happen locally\n",
    "del predictor\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(model_data=model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.6.0',\n",
    "                     py_version='py36',\n",
    "                     entry_point='serve.py', \n",
    "                     source_dir='scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a real instance_type to create a remote endpoint\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.large',\n",
    "                         serializer=json_serializer, deserializer=json_deserializer, content_type='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict( { \"url\": \"https://cdn1-www.cattime.com/assets/uploads/2011/12/file_2744_british-shorthair-460x290-460x290.jpg\" } )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the remote endpoint or it will cost you money\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
